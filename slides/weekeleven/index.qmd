---
title: "Computational Complexity"
description: "Discover how efficiency matters"
date: "2025-11-03"
date-format: long
author: Gregory M. Kapfhammer
execute:
  echo: true
format:
  live-revealjs:
    completion: true
    theme: default
    css: ../css/styles.css
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    footer: "Proofgrammers"
---

# Learning objectives

::: {.fragment .callout-note icon=true title="Learning Objectives for Theoretical Machines"}

- **CS-204-1**: Use both intuitive analysis and theoretical proof
techniques to correctly distinguish between problems that are tractable,
intractable, and uncomputable.
- **CS-204-2**: Correctly use one or more variants of the Turing machine
(TM) abstraction to both describe and analyze the solution to a
computational problem.
- **CS-204-3**: Correctly use one or more variants of the finite state
machine (FSM) abstraction to describe and analyze the solution to a
computational problem.
- **CS-204-4**: Use a formal proof technique to correctly classify a
problem according to whether or not it is in the P, NP, NP-Hard, and/or
NP-Complete complexity class(es).
- **CS-204-5**: Apply insights from theoretical proofs concerning the
limits of either program feasibility or complexity to the implementation
of both correct and efficient real-world Python programs.

:::

# Why computational complexity matters

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- Theory aids with learning objectives **CS-204-1** and **CS-204-4**
- Distinguish tractable from intractable problems
- Why some problems efficiently solvable while others impractical?
- How do we measure and compare algorithm efficiency?
- How do we classify problems by their complexity class?

:::

## Why computational complexity?

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- {{< iconify fa6-solid clock >}} **Time matters**: Some algorithms are fast,
others impossibly slow
- {{< iconify fa6-solid scale-balanced >}} **Comparing efficiency**: Formal ways
to compare algorithms
- {{< iconify fa6-solid chart-line >}} **Growth rates**: How does running time
change with input size?
- {{< iconify fa6-solid key >}} **Cryptography**: Security depends on
computational hardness
- {{< iconify fa6-solid microscope >}} **Question**: What problems are
efficiently solvable?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.85em;"}

- **Complexity distinguishes feasible from infeasible computation**
- Complexity reveals both theoretical and practical limits
- Exponential algorithms may work sometimes but are often unusable!

:::

## Review: finite automata

::: {.incremental style="margin-top: -0.5em; font-size: 0.9em;"}

- **Finite automata**: Restricted Turing machines with limited memory
- **DFA vs NFA**: Deterministic versus nondeterministic computation
- **Regular languages**: Languages recognized by finite automata
- **Regular expressions**: Compact notation for pattern matching
- **Equivalence**: DFAs, NFAs, and regex recognize same language class
- **Limitations**: Cannot count or maintain unbounded memory

:::

::: {.fragment .boxed-content .fade style="font-size: 0.875em;"}

- **Okay, finite automata showed computational restrictions!**
- **Now time restrictions: how quickly can problems be solved?**

:::

## Review: Pattern recognition efficiency

::: {.incremental style="margin-top: -0.5em; font-size: 0.85em;"}

- **Linear time**: DFAs process input in $O(n)$ time
- **Single pass**: Read input once, left to right
- **No backtracking**: Decisions made immediately
- **Applications**: Lexical analysis, pattern matching, protocol
verification
- **Connection to complexity**: Automata provide efficient solutions for
restricted problems. So, how else can restriction prove beneficial?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.825em;"}

- **Automata theory connects to and motivates complexity theory**
- Finite automata solve their problems efficiently (i.e., in linear time)
- Now we ask: which problems need more time? How much more?

:::

## Algorithm efficiency challenges 

::: {.columns style="margin-top: 1.25em;"}

::: {.fragment .column .fade style="margin-top: 0.25em;"}

### Different Inputs

- Varying sizes of input data
- Different types of data
- Degrees of sortedness
- Degrees of duplication
- Different data distributions

:::

::: {.fragment .column style="margin-top: 0.25em;"}

### Different Computers

- Diverse hardware setups
- Varying system performance
- Diverse system building
- Different operating systems
- Variation in system load

:::

:::

::: {.fragment .fade .boxed-content style="margin-top: -0.25em; font-size: 0.9em;"}

**Ultimate goal**: rigorous analytical evaluation method that yields actionable
insights that supports *understanding* and *prediction*

:::

# Worst-case time

::: {.incremental style="margin-top: -0.5em; font-size: 0.85em;"}

- {{< iconify fa6-solid chart-line >}} **Growth rates matter**: How
running time changes with input size $n$
- {{< iconify fa6-solid microscope >}} **Asymptotic analysis**: Focus on
behavior for large $n$, ignore constants
- {{< iconify fa6-solid scale-balanced >}} **Polynomial vs exponential**:
Fundamental distinction for tractability
- {{< iconify fa6-solid rocket >}} **Big-O notation**: Formalize and
compare algorithm efficiency

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Understanding growth rates is crucial**: an algorithm taking $100n$
steps is fundamentally faster than one taking $n^2$ steps for large $n$.
Asymptotic analysis lets proofgrammers compare algorithms meaningfully!

:::

## Understanding algorithm efficiency

::: {.incremental style="margin-top: -0.25em; font-size: 0.85em;"}

- {{< iconify fa6-solid stopwatch >}} **Absolute time**: Depends on hardware,
language, implementation
- {{< iconify fa6-solid chart-line >}} **Growth rate**: How time changes with
input size $n$
- {{< iconify fa6-solid microscope >}} **Asymptotic analysis**: Focus on
behavior for large $n$
- {{< iconify fa6-solid scale-balanced >}} **Comparing algorithms**: Growth rate
matters more than constants
- {{< iconify fa6-solid rocket >}} **Question**: How to effectively formalize
growth rates?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.85em;"}

- **Asymptotic analysis ignores constant factors and focuses on growth**
- Since absolute time varies by hardware and implementation, focus on worst-case
time complexity to enable meaningful comparisons across machines and
implementations. **Okay, let's dive into growth rates**!

:::

## Vast gulf between growth rates

![Growth rate comparison](10-complexity-theory-basics_0.png)

::: fragment

- **Insight**: Wow, these growth rates differ dramatically!
- **Insight**: Polynomial growth is feasible, exponential is not!

:::

## Exponential versus polynomial

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- **Polynomial time**: $n$, $n^2$, $n^3$, $n^{100}$ (tractable)
- **Exponential time**: $2^n$, $3^n$, $n!$ (intractable for large $n$)
- **Key insight**: Polynomial is feasible, exponential is not
- **Example**: $2^{100} \approx 10^{30}$ versus $100^2 = 10,000$
- **Cryptography**: Security relies on exponential-time attacks

:::

::: {.fragment .boxed-content .fade style="font-size: 0.825em;"}

- **The polynomial/exponential boundary defines tractability**
- Polynomial-time problems are considered efficiently computable
- Exponential-time problems are generally computationally impractical

:::

# Grasp Big-O notation

::: {.incremental style="margin-top: -0.5em; font-size: 0.825em;"}

- {{< iconify fa6-solid function >}} **Upper bound**: Big-O provides a formal
upper bound on growth rate
- {{< iconify fa6-solid filter >}} **Dominant terms**: Keep only the fastest
growing term, drop constants
- {{< iconify fa6-solid graduation-cap >}} **Standard language**: Main notation
for discussing algorithm efficiency
- {{< iconify fa6-solid circle-exclamation >}} **Common mistakes**: Input size
vs value, arithmetic operation costs

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Big-O notation standardizes efficiency discussions**: $5n^2 + 100n$
becomes $O(n^2)$ because we focus on dominant terms and ignore constants.
This enables meaningful comparisons across implementations and machines!

:::

## Formalizing asymptotic growth

::: {.incremental style="margin-top: -0.25em; font-size: 0.925em;"}

- {{< iconify fa6-solid book-open >}} **Big-O notation**: Upper bound on
growth rate
- {{< iconify fa6-solid filter >}} **Dominant terms**: Keep fastest
growing term
- {{< iconify fa6-solid users-gear >}} **Drop constants**: $5n^2$ becomes
$O(n^2)$
- {{< iconify fa6-solid rocket >}} **Drop lower terms**: $n^2 + 100n$
becomes $O(n^2)$
- {{< iconify fa6-solid microscope >}} **Question**: How to identify
dominant terms?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.8em;"}

- **Big-O provides a standard language for discussing efficiency**
- Lets proofgrammers communicate about algorithm performance
- Don't worry about implementation details or hardware differences since the
focus is on growth rates of functions describing algorithms!

:::

## Dominant term hierarchy

![Common complexity classes](10-complexity-theory-basics_2.png)

::: {.fragment .fade-right style="font-size: 0.825em;"}

- Dominant terms define the growth rates, with many easy to compare
- Form a hierarchy based on "common-sense rules" about growth rates

:::

## Simplifying to dominant terms

![Dominant term examples](10-complexity-theory-basics_4.png)

::: fragment

- For a function $f(n)$, define the dominant term as $DT(f)$
- The $DT$ captures the "fastest growing part" of $f(n)$
- Makes growth rates more clear, can see algorithm efficiency

:::

## Practical definition of Big-O notation

![Practical Big-O definition](10-complexity-theory-basics_6.png)

::: {.fragment .fade style="font-size: 0.715em;"}

- $f(n) = 3n + 5n \times logn$, $DT(f) = n \times logn$ and $f(n) \in O(n \times logn)$
- $g(n) = 3n \times logn^2$, $DT(g) = n \times logn^2$ and $g(n) \in O(n \times logn^2)$
- Also conclude that $f(n) \in O(g)$ since $n \times logn$ grows slower than $n \times logn^2$
- Example illustrates how big-O captures growth rate comparisons effectively
- Notice that it captures an "upper bound" on an algorithm's running time
- **Key task**: ignore constants and lower terms to find the dominant term
- **Important question**: how do you understand the meaning of $f \in O(g)$?

:::

## Ignore constants and lower terms

![Practical Big-O examples](10-complexity-theory-basics_7.png)

::: {.fragment .fade-right style="font-size: 0.825em;"}

- Determine the growth function for an algorithm as $f$
- Identify the dominant term $DT(f)$
- Express the complexity as $O(DT(f))$
- Capture the essential growth behavior of the algorithm
- Slow growth rates indicate efficient algorithms

:::

## Formal definition of Big-O notation

![Formal Big-O definition](10-complexity-theory-basics_8.png)

::: {.fragment .fade style="font-size: 0.75em;"}

- $f(n) \in O(g)$ means constants $c$ and $N$ exist
- The main constraint is $f(n) \leq c \times g(n)$ for all $n \geq N$

:::

## Some common Big-O mistakes

::: {.incremental style="margin-top: -0.25em; font-size: 0.825em;"}

- {{< iconify fa6-solid exclamation-triangle >}} **Mistake 1**: Confusing input
size with input value
    - Input size $n$ is length in symbols
    - Input value $M$ is the number represented
    - Example: Number 1000 has size $n=4$ but value $M=1000$
- {{< iconify fa6-solid exclamation-triangle >}} **Mistake 2**: Assuming all
operations are constant time
    - Arithmetic on large numbers is not $O(1)$
    - Multiplication is $O(n^2)$, division is $O(n^2)$
    - Addition and subtraction are $O(n)$
- These mistakes may lead to incorrect complexity analysis
- Remember that arithmetic complexity depends on "number size"

:::

# Running time analysis

::: {.incremental style="margin-top: -0.5em; font-size: 0.8em;"}

- {{< iconify fa6-solid code >}} **Practical examples**: Analyze Python
programs for complexity
- {{< iconify fa6-solid repeat >}} **Loop analysis**: Nested loops often
show quadratic or higher complexity
- {{< iconify fa6-solid search >}} **String operations**: Common
methods like substring search have linear time
- {{< iconify fa6-solid robot >}} **Turing machines**: Formal running
time analysis extends to prior models

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Running time analysis reveals algorithm efficiency**: count how many times the
innermost operations execute as a function of input size $n$. This skill helps
proofgrammers write faster, more scalable Python programs! For a Turing machine,
you can count how many transitions you follow!

:::

## Analyzing a simple nested loop

```{pyodide}
#| autorun: true
#| max-lines: 18
from typing import List
def duplicates(input_list: List[int]) -> bool:
    """Determine if the list contains a duplicate value."""
    n = len(input_list)
    for i in range(n):
        for j in range(n):
            if i != j and input_list[i] == input_list[j]:
                return True
    return False

# test cases
assert(duplicates([1,2,6,3,4,5,6,7,8]))
assert(not duplicates([1,2,3,4]))
print(f"duplicates([1,2,6,3,4,5,6,7,8]) = {duplicates([1,2,6,3,4,5,6,7,8])}")
print(f"duplicates([1,2,3,4]) = {duplicates([1,2,3,4])}")
```

## Linear-time string search

```{pyodide}
#| autorun: true
#| max-lines: 8
def contains_gaga(s: str) -> str:
    """Check if string contains 'GAGA' as substring."""
    if 'GAGA' in s:
        return 'yes'
    else:
        return 'no'

# test cases
print(f"contains_gaga('GAGAGA') = {contains_gaga('GAGAGA')}")
print(f"contains_gaga('HELLO') = {contains_gaga('HELLO')}")
print(f"contains_gaga('XGAGAY') = {contains_gaga('XGAGAY')}")
```

::: {.fragment .fade style="font-size: 0.875em;"}

- **Analysis**: String search examines each position once
- For input length $n$, `contains_gaga` is $O(n)$ or linear time!
- Importantly, it is much faster than $O(n^2)$ for large inputs!

:::

## Turing machine running time analysis

![containsGAGA Turing machine](10-complexity-theory-basics_11.png)

## Exact versus asymptotic time

![Exact running time calculation](10-complexity-theory-basics_13.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Calculating exact running times is tedious and unnecessary**
- Instead, use asymptotic estimates with Big-O notation
- Focus on dominant terms and growth rates rather than exact step counts

:::

## Formal definition of running time

![Running time definition](10-complexity-theory-basics_14.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Key points**: Maximum over all inputs of given length (worst case)
- "Computational step" means single CPU instruction for Python
- Or single transition for Turing machines

:::

## Python operation costs

![Python operation costs](10-complexity-theory-basics_15.png)

## Exercise: Analyze this Python program

![Python analysis exercise](10-complexity-theory-basics_16.png)

# Input size versus input value

::: {.incremental style="margin-top: -0.5em; font-size: 0.9em;"}

- {{< iconify fa6-solid ruler >}} **Critical distinction**: Input size
$n$ is length in symbols, not the number's value
- {{< iconify fa6-solid calculator >}} **Arithmetic costs**: Operations
on large numbers are not constant time
- {{< iconify fa6-solid explosion >}} **Exponential trap**: Loop running
$M$ times is exponential in $n = \log M$
- {{< iconify fa6-solid lock >}} **Cryptography connection**: Factoring
large numbers takes exponential time

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Don't confuse size with value**: the number 1000 has size $n=4$ digits
but value $M=1000$. A loop running $M$ times is exponential in $n$, not
linear. This mistake leads to incorrect complexity analysis!

:::

## Deceptive program analysis

![Trick question program](10-complexity-theory-basics_17.png)

## Size versus value distinction

![Input size versus value](10-complexity-theory-basics_18.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Critical insight**: Input size $n$ is length of input in symbols
- Input value $M$ is the number represented
- The loop runs $M$ times, not $n$ times
- This is exponential in $n$!

:::

## Size versus value examples

![Size vs value examples](10-complexity-theory-basics_19.png)

## Arithmetic operation complexity

![Arithmetic complexity](10-complexity-theory-basics_20.png)

## Arithmetic operation complexities

![Arithmetic operation costs](10-complexity-theory-basics_21.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Addition/subtraction**: $O(n)$ where $n$ is number of digits
- **Multiplication**: $O(n^2)$ using standard algorithm
- **Division**: $O(n^2)$
- These costs matter when analyzing algorithms on large numbers

:::

## Factoring: An exponential-time problem

![Factoring algorithm](10-complexity-theory-basics_22.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Factoring complexity**: Loop runs up to $M$ iterations
- Input size is $n = \log M$ digits
- Time complexity: $O(M) = O(2^n)$ exponential
- This exponential difficulty underlies RSA cryptography security

:::

## Factoring example with Python

```{pyodide}
#| autorun: true
#| max-lines: 20
def factor(N: int) -> str:
    """Find smallest factor of N greater than 1."""
    for i in range(2, N):
        if N % i == 0:
            return str(i)
    return str(N)

# test small numbers (fast)
print(f"factor(15) = {factor(15)}")
print(f"factor(91) = {factor(91)}")
print(f"factor(17) = {factor(17)}")

# larger numbers take exponentially longer!
# try factor(1000000007) - takes very long time
```

::: {.fragment .fade style="font-size: 0.875em;"}

- **Observe**: Small inputs are fast
- But doubling the number of digits roughly squares the time
- This exponential growth makes factoring large numbers infeasible

:::

# Complexity classes

::: {.incremental style="margin-top: -0.5em; font-size: 0.9em;"}

- {{< iconify fa6-solid equals >}} **Model equivalence**: Turing
machines, Python, and real computers are polynomially equivalent
- {{< iconify fa6-solid layer-group >}} **Organizing problems**:
Complexity classes group problems by resource requirements
- {{< iconify fa6-solid check-circle >}} **Poly vs Expo**: Tractable
versus intractable problems
- {{< iconify fa6-solid question >}} **Open questions**: P versus NP and
the limits of efficient computation

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Complexity classes organize computational difficulty**: **Poly**
contains tractable problems, **Expo** contains intractable ones. The
polynomial/exponential boundary determines practical solvability!

:::

## Model equivalence

![Computational model equivalence](10-complexity-theory-basics_23.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Key result**: Classical models simulate each other with only
polynomial overhead
- Turing machines, Python, and real computers are equivalent
- From now on, we use Python programs as our standard model
- Equivalent to random-access TM and real computers up to polynomial
factors

:::

## Defining complexity classes

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- **Complexity class**: Collection of problems sharing complexity
properties
- **Const**: Problems solvable in constant time $O(1)$
- **Lin**: Problems solvable in linear time $O(n)$
- **Quad**: Problems solvable in quadratic time $O(n^2)$
- **Poly**: Problems solvable in polynomial time $O(n^k)$
- **Expo**: Problems solvable in exponential time $O(2^n)$
- **PolyCheck**: Problems whose solutions are verifiable in polynomial
time

:::

::: {.fragment .fade style="font-size: 0.875em;"}

- **Focus on Poly, Expo, and PolyCheck**: These classes are
model-independent
- They capture fundamental distinctions
- Later chapters explore P, NP, and NP-completeness in depth

:::

## Tractable versus intractable

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- **Tractable**: Problems in **Poly** (polynomial time)
    - Examples: Sorting, searching, graph connectivity
    - Feasible for large inputs
- **Intractable**: Problems requiring exponential time
    - Examples: Factoring (no known polynomial algorithm)
    - Infeasible for large inputs
- **Open questions**: Many problems have unknown complexity
    - Is factoring in Poly?
    - Is every problem in PolyCheck also in Poly? (P vs NP!)

:::

::: {.fragment .fade style="font-size: 0.875em;"}

- {{< iconify fa6-solid lightbulb >}} **The polynomial/exponential
boundary is crucial**
- Understanding which side a problem falls on determines practical
solvability
- Proofgrammers must recognize this distinction

:::

## Key insights for proofgrammers

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- {{< iconify fa6-solid check-circle >}} **Big-O notation**: Standard
language for algorithm efficiency
- {{< iconify fa6-solid check-circle >}} **Asymptotic analysis**: Focus
on growth rates, not constants
- {{< iconify fa6-solid check-circle >}} **Input size vs value**:
Critical distinction for correctness
- {{< iconify fa6-solid check-circle >}} **Arithmetic costs**: Not
constant time for large numbers
- {{< iconify fa6-solid check-circle >}} **Complexity classes**: Organize
problems by resource requirements
- {{< iconify fa6-solid check-circle >}} **Practical impact**:
Exponential algorithms are infeasible

:::

::: {.fragment .fade style="font-size: 0.875em;"}

- **Complexity theory provides essential tools for algorithm design**
- Understanding these concepts helps proofgrammers write efficient
programs
- Recognize when problems are inherently difficult
- Next: P, NP, and NP-completeness

:::
