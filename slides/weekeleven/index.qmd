---
title: "Computational Complexity"
description: "Discover how efficiency matters"
date: "2025-11-03"
date-format: long
author: Gregory M. Kapfhammer
execute:
  echo: true
format:
  live-revealjs:
    completion: true
    theme: default
    css: ../css/styles.css
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    footer: "Proofgrammers"
---

# Learning objectives

::: {.fragment .callout-note icon=true title="Learning Objectives for Theoretical Machines"}

- **CS-204-1**: Use both intuitive analysis and theoretical proof
techniques to correctly distinguish between problems that are tractable,
intractable, and uncomputable.
- **CS-204-2**: Correctly use one or more variants of the Turing machine
(TM) abstraction to both describe and analyze the solution to a
computational problem.
- **CS-204-3**: Correctly use one or more variants of the finite state
machine (FSM) abstraction to describe and analyze the solution to a
computational problem.
- **CS-204-4**: Use a formal proof technique to correctly classify a
problem according to whether or not it is in the P, NP, NP-Hard, and/or
NP-Complete complexity class(es).
- **CS-204-5**: Apply insights from theoretical proofs concerning the
limits of either program feasibility or complexity to the implementation
of both correct and efficient real-world Python programs.

:::

# Why computational complexity matters

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- Theory aids with learning objectives **CS-204-1** and **CS-204-4**
- Distinguish tractable from intractable problems
- Why some problems efficiently solvable while others impractical?
- How do we measure and compare algorithm efficiency?
- How do we classify problems by their complexity class?

:::

## Why computational complexity?

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- {{< iconify fa6-solid clock >}} **Time matters**: Some algorithms are fast,
others impossibly slow
- {{< iconify fa6-solid scale-balanced >}} **Comparing efficiency**: Formal ways
to compare algorithms
- {{< iconify fa6-solid chart-line >}} **Growth rates**: How does running time
change with input size?
- {{< iconify fa6-solid key >}} **Cryptography**: Security depends on
computational hardness
- {{< iconify fa6-solid microscope >}} **Question**: What problems are
efficiently solvable?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.85em;"}

- **Complexity distinguishes feasible from infeasible computation**
- Computational complexity reveals theoretical and practical limits
- Exponential algorithms may work sometimes but are often unusable!

:::

## Review: finite automata

::: {.incremental style="margin-top: -0.5em; font-size: 0.9em;"}

- **Finite automata**: Restricted Turing machines with limited memory
- **DFA vs NFA**: Deterministic versus nondeterministic computation
- **Regular languages**: Languages recognized by finite automata
- **Regular expressions**: Compact notation for pattern matching
- **Equivalence**: DFAs, NFAs, and regex recognize same language class
- **Limitations**: Cannot count or maintain unbounded memory

:::

::: {.fragment .boxed-content .fade style="font-size: 0.875em;"}

- **Okay, finite automata showed computational restrictions!**
- **Now time restrictions: how quickly can problems be solved?**

:::

## Review: Pattern recognition efficiency

::: {.incremental style="margin-top: -0.5em; font-size: 0.85em;"}

- **Linear time**: DFAs process input in $O(n)$ time
- **Single pass**: Read input once, left to right
- **No backtracking**: Decisions made immediately
- **Applications**: Lexical analysis, pattern matching, protocol
verification
- **Connection to complexity**: Automata provide efficient solutions for
restricted problems. So, how else can restriction prove beneficial?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.825em;"}

- **Automata theory connects to and motivates complexity theory**
- Finite automata solve their problems efficiently (i.e., in linear time)
- **Now we ask**: which problems need more time? How much more?

:::

## Algorithm efficiency challenges 

::: {.columns style="margin-top: 1.25em;"}

::: {.fragment .column .fade style="margin-top: 0.25em;"}

### Different Inputs

- Varying sizes of input data
- Different types of data
- Degrees of sortedness
- Degrees of duplication
- Different data distributions

:::

::: {.fragment .column style="margin-top: 0.25em;"}

### Different Computers

- Diverse hardware setups
- Varying system performance
- Diverse system building
- Different operating systems
- Variation in system load

:::

:::

::: {.fragment .fade .boxed-content style="margin-top: -0.25em; font-size: 0.9em;"}

{{< iconify fa6-solid rocket >}} **Ultimate goal**: rigorous analytical
evaluation method that yields actionable insights that supports *understanding*
and *prediction*

:::

# Worst-case time

::: {.incremental style="margin-top: -0.5em; font-size: 0.85em;"}

- {{< iconify fa6-solid chart-line >}} **Growth rates matter**: How
running time changes with input size $n$
- {{< iconify fa6-solid microscope >}} **Asymptotic analysis**: Focus on
behavior for large $n$, ignore constants
- {{< iconify fa6-solid scale-balanced >}} **Polynomial vs exponential**:
Fundamental distinction for tractability
- {{< iconify fa6-solid rocket >}} **Big-O notation**: Formalize and
compare algorithm efficiency

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Understanding growth rates is crucial**: an algorithm taking $100n$
steps is fundamentally faster than one taking $n^2$ steps for large $n$.
Asymptotic analysis lets proofgrammers compare algorithms meaningfully!

:::

## Understanding algorithm efficiency

::: {.incremental style="margin-top: -0.25em; font-size: 0.85em;"}

- {{< iconify fa6-solid stopwatch >}} **Absolute time**: Depends on hardware,
language, implementation
- {{< iconify fa6-solid chart-line >}} **Growth rate**: How time changes with
input size $n$
- {{< iconify fa6-solid microscope >}} **Asymptotic analysis**: Focus on
behavior for large $n$
- {{< iconify fa6-solid scale-balanced >}} **Comparing algorithms**: Growth rate
matters more than constants
- {{< iconify fa6-solid rocket >}} **Question**: How to effectively formalize
growth rates?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.85em;"}

- **Asymptotic analysis ignores constant factors and focuses on growth**
- Since absolute time varies by hardware and implementation, focus on worst-case
time complexity to enable meaningful comparisons across machines and
implementations. **Okay, let's dive into growth rates**!

:::

## Vast gulf between growth rates

![Growth rate comparison](10-complexity-theory-basics_0.png)

::: fragment

- **Insight**: Wow, these growth rates differ dramatically!
- **Insight**: Polynomial growth is feasible, exponential is not!

:::

## Exponential versus polynomial

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- **Polynomial time**: $n$, $n^2$, $n^3$, $n^{100}$ (i.e., broadly tractable)
- **Exponential time**: $2^n$, $3^n$, $n!$ (i.e., intractable for large $n$)
- **Key insight**: Polynomial is feasible, exponential is not
- **Numerical Example**: $2^{100} \approx 10^{30}$ versus $100^2 = 10,000$
- **Cryptography**: Security relies on exponential-time attacks

:::

::: {.fragment .boxed-content .fade style="font-size: 0.825em;"}

- Polynomial-time problems are considered "efficiently computable"
- Exponential-time problems are generally "computationally impractical"
- {{< iconify fa6-solid rocket >}} **The polynomial/exponential
boundary demarcates tractability**

:::

# Grasp Big-O notation

::: {.incremental style="margin-top: -0.5em; font-size: 0.825em;"}

- {{< iconify fa6-solid book-open >}} **Upper bound**: Big-O provides a formal
upper bound on growth rate
- {{< iconify fa6-solid filter >}} **Dominant terms**: Keep only the fastest
growing term, drop constants
- {{< iconify fa6-solid graduation-cap >}} **Standard language**: Main notation
for discussing algorithm efficiency
- {{< iconify fa6-solid circle-exclamation >}} **Common mistakes**: Input size
versus value, arithmetic operation costs

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Big-O notation standardizes efficiency discussions**: $5n^2 + 100n$
becomes $O(n^2)$ because we focus on dominant terms and ignore constants.
This enables meaningful comparisons across implementations and machines!

:::

## Formalizing asymptotic growth

::: {.incremental style="margin-top: -0.25em; font-size: 0.925em;"}

- {{< iconify fa6-solid book-open >}} **Big-O notation**: Upper bound on
growth rate
- {{< iconify fa6-solid filter >}} **Dominant terms**: Keep fastest
growing term
- {{< iconify fa6-solid users-gear >}} **Drop constants**: $5n^2$ becomes
$O(n^2)$
- {{< iconify fa6-solid rocket >}} **Drop lower terms**: $n^2 + 100n$
becomes $O(n^2)$
- {{< iconify fa6-solid microscope >}} **Key Question**: How to identify
dominant terms?

:::

::: {.fragment .boxed-content .fade style="font-size: 0.75em;"}

- **Big-O provides a "standard language" for discussing efficiency**
- It enables proofgrammers to communicate about algorithm performance
- Don't worry about implementation details or hardware differences since the
focus is on growth rates of functions describing algorithms!

:::

## Dominant term hierarchy

![Common complexity classes](10-complexity-theory-basics_2.png)

::: {.fragment .fade-right style="font-size: 0.825em;"}

- Dominant terms define the growth rates, with many easy to compare
- Form a hierarchy based on "common-sense rules" about growth rates

:::

## Simplifying to dominant terms

![Dominant term examples](10-complexity-theory-basics_4.png)

::: fragment

- For a function $f(n)$, define the dominant term as $DT(f)$
- The $DT(f)$ captures the "fastest growing part" of $f(n)$
- Makes growth rates more clear, can see algorithm efficiency!

:::

## Practical definition of Big-O notation

![Practical Big-O definition](10-complexity-theory-basics_6.png)

::: {.fragment .fade style="font-size: 0.7em;"}

- $f(n) = 3n + 5n \times logn$, $DT(f) = n \times logn$ and $f(n) \in O(n \times logn)$
- $g(n) = 3n \times logn^2$, $DT(g) = n \times logn^2$ and $g(n) \in O(n \times logn^2)$
- Also conclude that $f(n) \in O(g)$ since $n \times logn$ grows slower than $n \times logn^2$
- Example illustrates how big-O captures growth rate comparisons effectively
- Notice that it captures an "upper bound" on an algorithm's running time
- **Key task**: ignore constants and lower terms to find the dominant term
- **Important question**: how do you understand the meaning of $f \in O(g)$?

:::

## Ignore constants and lower terms

![Practical Big-O examples](10-complexity-theory-basics_7.png)

::: {.fragment .fade-right style="font-size: 0.825em;"}

- Determine the growth function for an algorithm as $f$
- Identify growth functions dominant term as $DT(f)$
- Express the computational complexity as $O(DT(f))$
- Capture the essential growth behavior of the algorithm
- **Remember that slow growth rates indicate efficient algorithms**!

:::

## Formal definition of Big-O notation

![Formal Big-O definition](10-complexity-theory-basics_8.png)

::: {.fragment .fade style="font-size: 0.85em;"}

- $f(n) \in O(g)$ means constants $c$ and $N$ exist
- The main constraint is $f(n) \leq c \times g(n)$ for all $n \geq N$

:::

## Some common Big-O mistakes

::: {.incremental style="margin-top: -0.25em; font-size: 0.825em;"}

- {{< iconify fa6-solid circle-exclamation >}} **Mistake 1**: Confusing input
size with input value
    - Input size $n$ is "length in symbols"
    - Input value $M$ is the number represented
    - Example: Number 1000 has size $n=4$ but value $M=1000$
- {{< iconify fa6-solid circle-exclamation >}} **Mistake 2**: Assuming all
operations are constant time
    - Arithmetic on large numbers is not $O(1)$
    - Multiplication is $O(n^2)$, division is $O(n^2)$
    - Addition and subtraction are $O(n)$
- These mistakes may lead to incorrect complexity analysis
- Remember that arithmetic complexity depends on "number size"

:::

# Running time analysis

::: {.incremental style="margin-top: -0.5em; font-size: 0.8em;"}

- {{< iconify fa6-solid code >}} **Practical examples**: Analyze Python
programs for complexity
- {{< iconify fa6-solid repeat >}} **Loop analysis**: Nested loops often show
quadratic or higher complexity
- {{< iconify fa6-solid rocket >}} **String operations**: Methods like
substring search are linear time
- {{< iconify fa6-solid robot >}} **Turing machines**: Formal running time
analysis extends to prior models

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

**Running time analysis reveals algorithm efficiency**: count how many times
the innermost operations execute as a function of input size $n$. This skill
helps proofgrammers write faster, more scalable Python programs. Or, for a
Turing machine, you can count how many transitions you follow! **In both cases,
make sure to "count" from the perspective of the worst-case input**!

:::

## Analyzing a simple nested loop {transition="convex"}

```{pyodide}
#| autorun: true
#| max-lines: 18
from typing import List
def duplicates(input_list: List[int]) -> bool:
    """Determine if the list contains a duplicate value."""
    n = len(input_list)
    for i in range(n):
        for j in range(n):
            if i != j and input_list[i] == input_list[j]:
                return True
    return False

# test cases
assert(duplicates([1,2,6,3,4,5,6,7,8]))
assert(not duplicates([1,2,3,4]))
print(f"duplicates([1,2,6,3,4,5,6,7,8]) = {duplicates([1,2,6,3,4,5,6,7,8])}")
print(f"duplicates([1,2,3,4]) = {duplicates([1,2,3,4])}")
```

## Linear-time string search {transition="convex"}

```{pyodide}
#| autorun: true
#| max-lines: 8
def contains_gaga(s: str) -> str:
    """Check if string contains 'GAGA' as substring."""
    if 'GAGA' in s:
        return 'yes'
    else:
        return 'no'

# test cases
print(f"contains_gaga('GAGAGA') = {contains_gaga('GAGAGA')}")
print(f"contains_gaga('HELLO') = {contains_gaga('HELLO')}")
print(f"contains_gaga('XGAGAY') = {contains_gaga('XGAGAY')}")
```

::: {.fragment .fade style="font-size: 0.875em;"}

- **Analysis**: String search examines each position once
- For input length $n$, `contains_gaga` is $O(n)$ or linear time!
- Importantly, it is much faster than $O(n^2)$ for large inputs!

:::

## Turing machine running time analysis

![containsGAGA Turing machine](10-complexity-theory-basics_11.png)

## Formal definition of running time

![Running time definition](10-complexity-theory-basics_14.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Key points**: Maximum over all inputs of given length or "worst case"
- "Computational step" means single CPU instruction for Python
- Or, it could mean taking a single transition for Turing machines
- Look at running time and then consider the asymptotic costs!

:::

## Python operation costs

![](10-complexity-theory-basics_15.png)

## Exercise: Analyze this Python program

![Python analysis exercise](10-complexity-theory-basics_16.png)

## Run `matchingCharIndices` {transition="convex"}

```{pyodide}
#| autorun: true
#| max-lines: 10
def matchingCharIndices(inString): 
    # split the input into a list of 2 words
    (word1, word2) = inString.split(' ') 
    # create an empty list that will store pairs of indices
    pairs = [ ] 
    # append every relevant pair of indices to the pairs list
    for i in range(len(word1)): 
        for j in range(len(word2)):
            if word1[i] == word2[j]: 
                thisPair = str(i) + ',' + str(j) 
                pairs.append(thisPair) 
    # concatenate all the pairs together, separated by space characters
    return ' '.join(pairs) 

def testMatchingCharIndices():
    testVals = [('bat ball', '0,0 1,1'),
                ('hello world', '2,3 3,3 4,1'),
                ('a b', ''),
                ('aaa aaaaa', '0,0 0,1 0,2 0,3 0,4 1,0 1,1 1,2 1,3 1,4 2,0 2,1 2,2 2,3 2,4'),
                ('abcdefgh fghijklmnop', '5,0 6,1 7,2'),
    ]
    for (inString, solution) in testVals:
        val = matchingCharIndices(inString)
        print(inString, ':', val)
        assert val == solution

testMatchingCharIndices()
```

::: {.fragment style="font-size: 0.8em;"}

- {{< iconify fa6-solid repeat >}} **Run this program with different inputs and
understand how it works**!

:::

## Initial complexity analysis of `matchingCharIndices`

::: {.incremental style="margin-top: -0.25em; font-size: 0.75em;"}

- {{< iconify fa6-solid code >}} **Function purpose**: Find matching character
indices between two words
- {{< iconify fa6-solid repeat >}} **Nested loops**: Outer loop iterates
`len(word1)` times, inner `len(word2)` times
- {{< iconify fa6-solid calculator >}} **Total iterations**: $n_1 \times n_2$
where $n_1 =$ `len(word1)`, $n_2 =$ `len(word2)`
- {{< iconify fa6-solid chart-line >}} **Worst-case analysis**: If both words
have equal length $\approx n/2$
- {{< iconify fa6-solid lightbulb >}} **But wait**: What about the work done
*inside* each iteration?

:::

::: {.fragment .fade .boxed-content style="font-size: 0.7em;"}

{{< iconify fa6-solid lightbulb >}} **The nested loops suggest $O(n²)$
complexity**: With input `'aaa aaaaa'`, the function performs $3 \times 5 = 15$
iterations. While this analysis is not "wrong" per se it does not give the
complete picture! The full analysis requires examining what happens *inside*
each iteration, particularly the string construction operations! **Let's
explore further**!

:::

## Complete analysis: Why $O(n² log n)$?

::: {.incremental style="margin-top: -0.25em; font-size: 0.725em;"}

- {{< iconify fa6-solid magnifying-glass >}} **Careful examination**: The
nested loops are only part of the story!
- {{< iconify fa6-solid text-width >}} **String construction**: Line 10
creates `thisPair = str(i) + ',' + str(j)`. 
- {{< iconify fa6-solid hashtag >}} **Integer-to-string**:
Converting integer `i` or `j` to string requires $O(log n)$ time
- {{< iconify fa6-solid list-ol >}} **Why logarithmic?**: Number of digits in
integer `k` is $O(log_{₁₀} k)$, so `str(k)` takes $O(log k)$
time. And, importantly, the cost of `append` is $O(1)$!
- {{< iconify fa6-solid book >}} **Combined**: $O(n²)$
iterations times $O(log n)$ per iteration = **$O(n² log n)$**
- **Here is the full breakdown for the line ranges in book's code**:
    - Lines 3-5: $O(n)$
    - Lines 7-11: $O(n² log n)$
    - Lines 13: $O(n² log n)$
    - Overall: $O(n + n² log n + n² log n)$
    - Simplified: $O(n² log n)$

:::

## String operations are not free! 

::: {.fragment style="margin-top: -0.5em; font-size: 0.925em;"}

- For a list in Python called `L` the length is `len(L)`
- It is more difficult to understand the length of an `int`!
- The integer 999 has 3 digits, requiring $O(log 1000) = O(3)$
time to convert and/or process this variable as a string
- For input size $n$, indices can reach $n$, requiring $O(log n)$ time
per conversion and/or processing of the string
- Accordingly, this logarithmic factor multiplies the quadratic loop structure,
yielding $O(n² log n)$ total complexity!
- **Key insight: Not sufficient to _only_ look at the loop structure!**
- **What is distinction between "size" and "value" of an input?**

:::

# Input size versus value

::: {.incremental style="margin-top: -0.5em; font-size: 0.8em;"}

- {{< iconify fa6-solid ruler >}} **Critical distinction**: Input size
$n$ is length in symbols, not number's value
- {{< iconify fa6-solid calculator >}} **Arithmetic costs**: Operations
on large numbers are not constant time
- {{< iconify fa6-solid explosion >}} **Exponential trap**: Loop running
$M$ times is exponential in $n = \log M$
- {{< iconify fa6-solid lock >}} **Cryptography**: Factoring
large numbers takes exponential time

:::

::: {.fragment .fade .boxed-content style="font-size: 0.85em;"}

**Don't confuse size with value**: the number 1000 has size $n=4$ digits
but value $M=1000$. A loop running $M$ times is exponential in $n$, not
linear. This mistake leads to incorrect complexity analysis! See the book's
discussion of the `MCopiesOfC.py` program for more details!

:::

## Deceptive program analysis

![Trick question program called `MCopiesOfC.py`](10-complexity-theory-basics_17.png)

::: fragment

- Start with an empty list and append character 'C' $M$ times
- This program as a whole requires $O(M)$ time, ... or not?
- Actually, the running time is $O(10^n)$ in input size $n$!

:::

## Size versus value distinction

![Input size versus value for numbers in decimal or binary
notation](10-complexity-theory-basics_18.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Critical insight**: Input size $n$ is length of input in symbols
- Input value $M$ is the number represented in a notation
- To be clear, the loop runs $M$ times, not $n$ times
- However, this is exponential in $n$ when using binary notation!
- Subtle, but important distinction for complexity analysis!

:::

## Size versus value examples

![Size versus value examples for integers](10-complexity-theory-basics_19.png)

## Arithmetic operation complexity

![Arithmetic complexity](10-complexity-theory-basics_20.png)

## Factoring is exponential-time

![Factoring algorithm](10-complexity-theory-basics_22.png)

::: {.fragment .fade style="font-size: 0.875em;"}

- **Factoring complexity**: Loop runs up to $M$ iterations
- Input size is $n = \log M$ digits
- Time complexity: $O(M) = O(2^n)$ exponential
- This exponential difficulty underlies RSA cryptography security

:::

## Factoring example with Python

```{pyodide}
#| autorun: true
#| max-lines: 10
def factor(N: int) -> str:
    """Find smallest factor of N greater than 1."""
    for i in range(2, N):
        if N % i == 0:
            return str(i)
    return str(N)

# test small numbers (fast)
print(f"factor(15) = {factor(15)}")
print(f"factor(91) = {factor(91)}")
print(f"factor(17) = {factor(17)}")
```

::: {.fragment .fade style="font-size: 0.8em;"}

- Small inputs fast, but doubling the number of digits increases time
- This exponential growth makes factoring large numbers infeasible

:::

# Aha! Time complexity is measured as a function of the length of the input, not the numerical value(s) of the input!

## Complexity classes

::: {.incremental style="margin-top: -0.5em; font-size: 0.95em;"}

- {{< iconify fa6-solid circle-exclamation >}} **Model distinctions**: From the
lens of complexity, Turing machines are "less efficient" than typical computer
- {{< iconify fa6-solid equals >}} **Model equivalence**: Turing machines,
Python, and real computers are "polynomially equivalent"
- {{< iconify fa6-solid layer-group >}} **Organizing problems**: After picking a
model, complexity classes group problems by resource requirements
- {{< iconify fa6-solid circle-check >}} **Poly versus Expo**: Tractable versus
intractable problems

:::

::: {.fragment .fade .boxed-content style="font-size: 0.8em;"}

{{< iconify fa6-solid lightbulb >}} **Complexity classes organize computational
difficulty**: **Poly** contains the "tractable" problems, **Expo** contains
"intractable" ones. This means that the polynomial/exponential boundary
determines practical solvability!

:::

## Model equivalence

![Computational model
equivalence](10-complexity-theory-basics_23.png){.sensible-size-thick-border} 

## Classical models simulate each other with only polynomial overhead

::: {.fragment .fade style="font-size: 0.95em;"}

- Turing machines, Python, and real computers are "equivalent"
- However, Turing machines tend to be harder to "program"
- From now on, we use Python programs as our standard model
- Python programs are a good "abstraction" for Turing machines
- Python programs are equivalent to random-access Turing machine and real
computers up to polynomial factors
- **With that in mind, what are the other complexity classes**?
- {{< iconify fa6-solid rocket >}} **Let's briefly explore these classes in
greater detail**!

:::

## Defining complexity classes

::: {.incremental style="margin-top: -0.25em; font-size: 0.8em;"}

- **Complexity class**: Collection of problems sharing complexity
properties
- **Const**: Problems solvable in constant time $O(1)$
- **Lin**: Problems solvable in linear time $O(n)$
- **Quad**: Problems solvable in quadratic time $O(n^2)$
- **Poly**: Problems solvable in polynomial time $O(n^k)$
- **Expo**: Problems solvable in exponential time $O(2^n)$
- **PolyCheck**: Problems whose solutions are verifiable in polynomial
time

:::

::: {.fragment .boxed-content .fade style="font-size: 0.7em;"}

- **Focus on Poly, Expo, and PolyCheck**: These classes are "model independent"
- They capture fundamental distinctions, generalize by use of the variable $k$
- Later chapters explore P, NP, and NP-completeness in depth!

:::

## Tractable versus intractable

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- **Tractable**: Problems in **Poly** (polynomial time)
    - Examples: Sorting, searching, graph connectivity
    - Feasible for large inputs
- **Intractable**: Problems requiring exponential time
    - Examples: Factoring (no known polynomial algorithm)
    - Infeasible for large inputs
- **Open questions**: Many problems have unknown complexity
    - Is factoring in Poly?
    - Is every problem in PolyCheck also in Poly? (P vs NP!)

:::

## Key insights for proofgrammers

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- {{< iconify fa6-solid check-circle >}} **Big-O notation**: Standard
language for algorithm efficiency
- {{< iconify fa6-solid check-circle >}} **Asymptotic analysis**: Focus
on growth rates, not constants
- {{< iconify fa6-solid check-circle >}} **Input size vs value**:
Critical distinction for correctness
- {{< iconify fa6-solid check-circle >}} **Arithmetic costs**: Not
constant time for large numbers
- {{< iconify fa6-solid check-circle >}} **Complexity classes**: Organize
problems by resource requirements
- {{< iconify fa6-solid check-circle >}} **Practical impact**:
Exponential algorithms are infeasible

:::

::: {.fragment .boxed-content .fade style="font-size: 0.7em;"}

- **Complexity theory provides essential tools for algorithm design**
- Understanding these concepts helps proofgrammers write efficient programs
- Use these methods to recognize when problems are inherently difficult

:::
